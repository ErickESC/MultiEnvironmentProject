# --- START OF FILE trainObsDT.py ---

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset as TorchDataset
from transformers import (
    DecisionTransformerConfig,
    DecisionTransformerModel,
    Trainer,
    TrainingArguments,
    EvalPrediction, # Import EvalPrediction
)
# --- Add scikit-learn import for metrics ---
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
# --- End import ---
import numpy as np
import pickle
import random
import logging
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

# --- Configuration ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

NUM_ACTIONS_PER_DIM = [3, 3, 2]
NUM_ACTION_DIMS = len(NUM_ACTIONS_PER_DIM)
STATE_DIM = 54 # <<<--- Ensure this matches the data in your pickle file (54 or 7?)
CONTEXT_LENGTH = 20
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- Data Loading and Preprocessing (load_your_data, calculate_returns_to_go, process_dataset) ---
# ... (Keep these functions exactly as in the previous version) ...
def load_your_data(pickle_path,
                   cumulative_rewards_in_dataset=False):
    """Loads dataset from a pickle file."""
    logger.info(f"Loading dataset from: {pickle_path}")
    try:
        with open(pickle_path, 'rb') as f:
            dataset = pickle.load(f)
    except FileNotFoundError:
        logger.error(f"Dataset file not found at: {pickle_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading pickle file: {e}")
        raise

    observations_list = dataset['observations']
    actions_list = dataset['actions']
    rewards_list = dataset['rewards']
    dones_list = dataset['dones']

    num_episodes = len(observations_list)
    if not (len(actions_list) == num_episodes and len(rewards_list) == num_episodes and len(dones_-learn matplotlib
    ```
2.  **`trainObsDT.py` Modifications:**
    *   Import necessary metrics functions from `sklearn.metrics`.
    *   Define a `compute_metrics` function to calculate Accuracy and F1 Score per action dimension.
    *   Pass this function to the `MultiDiscreteDecisionTransformerTrainer` during initialization.
    *   Ensure `logging_dir` is set in `TrainingArguments` for TensorBoard logging.
    *   Add a `plot_metrics` function to generate plots from the training history after training completes.
    *   Call `plot_metrics` at the end of the main execution block.

---

**Updated `trainObsDT.py` (Incorporating Metrics and Plotting)**

```python
# --- START OF FILE trainObsDT.py ---

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset as TorchDataset
from transformers import (
    DecisionTransformerConfig,
    DecisionTransformerModel,
    Trainer,
    TrainingArguments,
    EvalPrediction, # Import EvalPrediction
)
# Import metrics and plotting libraries
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt
import numpy as np
import pickle
import random
import logging
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

# --- Configuration ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- ACTION SPACE & STATE CONFIGURATION (IMPORTANT!) ---
NUM_ACTIONS_PER_DIM = [3, 3, 2]  # Action space for SolidGame environment
NUM_ACTION_DIMS = len(NUM_ACTIONS_PER_DIM)
STATE_DIM = 54 # <<<--- Ensure this matches the data in your pickle file (54 or 7?)
CONTEXT_LENGTH = 20  # K: How many steps the model sees
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- 1. Data Loading and Preprocessing ---
# (load_your_data, calculate_returns_to_go, process_dataset remain the same as before)
# Ensure process_dataset returns state_mean, state_std

def load_your_data(pickle_path,
                   cumulative_rewards_in_dataset=False):
    """
    Loads dataset from a pickle file generated by the user's PPO script.
    Handles validation of observation dimensions.
    """
    logger.info(f"Loading dataset from: {pickle_path}")
    try:
        with open(pickle_path, 'rb') as f:
            dataset = pickle.load(f)
    except FileNotFoundError:
        logger.errorlist) == num_episodes):
        logger.error("Dataset lists have inconsistent lengths.")
        raise ValueError("Inconsistent number of episodes in dataset file.")

    processed_episodes = []
    all_final_scores = []

    logger.info(f"Processing {num_episodes} episodes...")
    for i in range(num_episodes):
        ep_obs = observations_list[i]
        ep_act = actions_list[i]
        ep_rew = rewards_list[i]
        ep_done = dones_list[i]

        ep_len = len(ep_obs)
        if not (len(ep_act) == ep_len and len(ep_rew) == ep_len and len(ep_done) == ep_len):
            logger.warning(f"Episode {i} inconsistent lengths. Skipping.")
            continue
        if ep_len == 0:
            logger.warning(f"Episode {i} is empty. Skipping.")
            continue

        episode_data = []
        current_episode_total_reward = 0.0
        for t in range(ep_len):
            try:
                observation = np.array(ep_obs[t], dtype=np.float32)
                if observation.shape != (STATE_DIM,):
                     raise ValueError(f"Obs shape mismatch: expected ({STATE_DIM},), got {observation.shape}")
            except Exception as e:
                logger.error(f"Skipping step {t} ep {i}: Invalid observation: {e}")
                continue

            try:
                action = np.array(ep_act[t], dtype=np.int64)
                if action.shape != (NUM_ACTION_DIMS,):
                    raise ValueError(f"Act shape mismatch: expected ({NUM_ACTION_DIMS},), got {action.shape}")
                for dim_idx, act_val in enumerate(action):
                    if not (0 <= act_val < NUM_ACTIONS_PER_DIM[dim_idx]):
                        raise(f"Dataset file not found at: {pickle_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading pickle file: {e}")
        raise

    observations_list = dataset['observations']
    actions_list = dataset['actions']
    rewards_list = dataset['rewards']
    dones_list = dataset['dones']

    num_episodes = len(observations_list)
    if not (len(actions_list) == num_episodes and len(rewards_list) == num_episodes and len(dones_list) == num_episodes):
        logger.error("Dataset lists have inconsistent lengths.")
        raise ValueError("Inconsistent number of episodes in dataset file.")

    processed_episodes = []
    all_final_scores = []

    logger.info(f"Processing {num_episodes} episodes...")
    for i in range(num_episodes):
        ep_obs = observations_list[i]
        ep_act = actions_list[i]
        ep_rew = rewards_list[i]
        ep_done = dones_list[i]

        ep_len = len(ep_obs)
        if not (len(ep_act) == ep_len and len(ep_rew) == ep_len and len(ep_done) == ep_len):
            logger.warning(f"Episode {i} has inconsistent lengths. Skipping.")
            continue
        if ep_len == 0:
            logger.warning(f"Episode {i} is empty. Skipping.")
             ValueError(f"Act value out of bounds dim {dim_idx}: {act_val}")
            except Exception as e:
                 logger.warning(f"Skipping step {t} ep {i}: Invalid action: {e}")
                 continue

            done = bool(ep_done[t])
            step_reward = 0.0
            raw_reward_t = ep_rew[t]
            try:
                if cumulative_rewards_in_dataset:
                    step_reward = float(raw_reward_t) if t == 0 else float(raw_reward_t) - float(ep_rew[t-1])
                else: step_reward = float(raw_reward_t)
            except (TypeError, ValueError) as e:
                 logger.warning(f"Reward conversion error ep {i} step {t}: {e}. Setting reward to 0.")
                 step_reward = 0.0

            current_episode_total_reward += step_reward
            episode_data.append({'observation': observation, 'action': action, 'reward': step_reward, 'done': donecontinue

        episode_data = []
        current_episode_total_reward = 0.0
        for t in range(ep_len):
            try:
                observation = np.array(ep_obs[t], dtype=np.float32)
                # !!! CRITICAL VALIDATION !!!
                if observation.shape != (STATE_DIM,):
                    raise ValueError(f"Observation shape mismatch at ep {i}, step {t}: expected ({STATE_DIM},), got {observation.shape}")
            except Exception as e:
                logger.error(f"Skipping step {t} in episode {i} due to invalid observation: {e}")
                continue

            try:
                action = np.array(ep_act[t], dtype=np.int64)
                if action.shape != (NUM_ACTION_DIMS,):
                    raise ValueError(f"Action shape mismatch:})

        if episode_data:
            processed_episodes.append(episode_data); all_final_scores.append(current_episode_total_reward)
        else: logger.warning(f"Episode {i} had no valid steps. Skipping.")

    if not processed_episodes: raise ValueError("No valid episodes processed.")

    avg_return = np.mean(all_final_scores) if all_final_scores else 0
    max_return = np.max(all_final_scores) if all_final_scores else  expected ({NUM_ACTION_DIMS},), got {action.shape}")
                for dim_idx, act_val in enumerate(action):
                    if not (0 <= act_val < NUM_ACTIONS_PER_DIM[dim_idx]):
                        raise ValueError(f"Action value out of bounds in dim {dim_idx}: got {act_val}, expected < {NUM_ACTIONS_PER_DIM[dim_idx]}")
            except Exception0
    logger.info(f"Loaded {len(processed_episodes)} episodes. Avg Return: {avg_return:.2f}, Max Return: {max_return:.2f}")
    logger.info(f"Suggest TARGET_RETURN around {avg_return:.2f}")
    return processed_episodes

def calculate_returns_ as e:
                 logger.warning(f"Skipping step {t} in episode {i} due to invalid action: {e}")
                 continue

            done = bool(ep_done[t])

            stepto_go(episode):
    rewards = np.array([step['reward'] for step in episode])
_reward = 0.0
            raw_reward_t = ep_rew[t]
            try    n = len(rewards)
    rtgs = np.zeros_like(rewards, dtype=np.float32:
                if cumulative_rewards_in_dataset:
                    if t == 0:
                        step_)
    cumulative_reward = 0
    for t in reversed(range(n)): cumulative_reward += rewardsreward = float(raw_reward_t)
                    else:
                        raw_reward_prev = ep_rew[t-1]
                        step_reward = float(raw_reward_t) - float(raw[t]; rtgs[t] = cumulative_reward
    return rtgs

def process_dataset(episodes, context_length):
    all_sequences = []
    max_ep_len = 0
_reward_prev)
                else:
                    step_reward = float(raw_reward_t)
    all_states = []
    logger.info("Processing episodes...")
    for episode_idx, episode in enumerate(ep            except (TypeError, ValueError) as e:
                 logger.warning(f"Reward conversion error at ep {i}, step {t}: {e}. Setting reward to 0.")
                 step_reward = 0.0

            current_episodeisodes):
        if not episode: logger.warning(f"Skipping empty episode {episode_idx}"); continue
        _total_reward += step_reward
            episode_data.append({
                'observation': observation, 'ep_len = len(episode); max_ep_len = max(max_ep_len, ep_action': action,
                'reward': step_reward, 'done': done
            })

        if episode_datalen)
        rtgs = calculate_returns_to_go(episode)
        for step in episode: all_states.append(step['observation'])
        for t in range(ep_len):
            start: # Only add if steps were successfully processed
            processed_episodes.append(episode_data)
            all_final_idx = max(0, t - context_length + 1); end_idx = t + 1_scores.append(current_episode_total_reward)
        else:
             logger.warning(f"Episode {i} resulted in no valid steps after cleaning. Skipping.")

    if not processed_episodes:
            seq_states = [step['observation'] for step in episode[start_idx:end_idx]]
        logger.error("No valid episodes found after processing.")
        raise ValueError("No episodes processed. Check STATE
            seq_actions = [step['action'] for step in episode[start_idx:end_idx]]_DIM and dataset integrity.")

    avg_return = np.mean(all_final_scores) if all
            seq_rtgs = rtgs[start_idx:end_idx]; seq_timesteps = np_final_scores else 0
    max_return = np.max(all_final_scores) if.arange(start_idx, end_idx)
            if t < ep_len - 1: target all_final_scores else 0
    logger.info(f"Loaded and processed {len(processed__action = episode[t+1]['action']
            else: continue
            padding_len = context_length - lenepisodes)} episodes.")
    logger.info(f"Calculated Average Episode Return from data: {avg_return:.2f}")
    logger.info(f"Calculated Max Episode Return from data: {max_(seq_states)
            padded_states = np.concatenate([np.zeros((padding_len, STATE_DIM), dtype=np.float32), np.array(seq_states, dtype=np.floatreturn:.2f}")
    logger.info(f"Suggest using TARGET_RETURN around {avg_return:.2f} for evaluation.")

    return processed_episodes


def calculate_returns_to_go(episode32)], axis=0)
            padded_actions = np.concatenate([np.full((padding_len, NUM_ACTION_DIMS), -100, dtype=np.int64), np.array):
    """Calculates returns-to-go for a single episode."""
    rewards = np.array([step['reward'] for step in episode])
    n = len(rewards)
    rtgs = np.(seq_actions, dtype=np.int64)], axis=0)
            padded_rtgs = np.concatenate([np.zeros(padding_len, dtype=np.float32), seq_rtgszeros_like(rewards, dtype=np.float32)
    cumulative_reward = 0
    for t in reversed(range(n)):
        cumulative_reward += rewards[t]
        rtgs[], axis=0)
            padded_timesteps = np.concatenate([np.zeros(padding_len, dtype=np.int64), seq_timesteps], axis=0)
            attention_mask = npt] = cumulative_reward
    return rtgs

def process_dataset(episodes, context_length):
    """Processes raw episodes into sequences suitable for Decision Transformer."""
    all_sequences = []
    max_.concatenate([np.zeros(padding_len, dtype=np.int64), np.ones(len(seq_states), dtype=np.int64)], axis=0)
            all_sequences.appendep_len = 0
    all_states = [] # Collect states ONLY from the training split for normalization

    logger.info("Processing episodes and calculating RTGs...")
    for episode_idx, episode in enumerate(episodes):
        if not episode:
            logger.warning(f"Skipping empty episode at index {episode_({"states": padded_states, "actions": padded_actions, "returns_to_go": padded_rtgs.reshape(-1, 1),
                                 "timesteps": padded_timesteps.reshape(-1, 1),idx} in process_dataset.")
            continue
        ep_len = len(episode)
        max_ep_len = max(max_ep_len, ep_len)
        rtgs = calculate_returns_to_ "attention_mask": attention_mask, "targets": target_action})
    logger.info(f"Created {len(all_sequences)} sequences. Max ep len: {max_ep_len}")
    if not all_statesgo(episode)

        for step in episode:
            # Append state only if it's not None or: raise ValueError("No states collected.")
    all_states_np = np.array(all_states); state_mean = np.mean(all_states_np, axis=0); state_std = np.std(all otherwise invalid
            if isinstance(step.get('observation'), np.ndarray):
                 all_states.append(step['observation'])
            else:
                 logger.warning(f"Invalid observation found in episode {episode_idx},_states_np, axis=0) + 1e-6
    for seq in all_sequences:
         step data: {step}")


        for t in range(ep_len):
            start_idx = max(0,valid = seq['attention_mask'] == 1
        norm_s = np.where(state_std > 1e-6, (seq['states'][valid] - state_mean) / state_std, 0 t - context_length + 1)
            end_idx = t + 1

            # Ensure all.0)
        seq['states'][valid] = norm_s; seq['states'][~valid] = steps have valid observations before proceeding
            # This requires checking during episode_data creation in load_your_data primarily
            try:
                seq_states = [step['observation'] for step in episode[start_idx:end_idx]]
                seq_actions = [step['action'] for step in episode[start_idx:end_ 0.0
    return all_sequences, max_ep_len, state_mean, state_std

# --- Dataset and Collator (DecisionTransformerDataset, DecisionTransformerDataCollator) ---
# ... (Keep these exactly as in the previous version) ...
class DecisionTransformerDataset(TorchDataset):
    def __init__(self, sequences): self.sequences = sequences
    def __len__(self): return len(self.sequences)
    def __getitemidx]]
            except KeyError as e:
                 logger.error(f"Missing key {e} in step data during sequence creation (ep {episode_idx}, t {t}). Check data loading.")
                 continue # Skip this sequence if data is malformed


            seq_rtgs = rtgs[start_idx:end_idx]
            seq_timesteps = np.arange(start_idx, end_idx)

            if t < ep__(self, idx):
        item = self.sequences[idx]
        return {"states": torch.tensor(item["states"], dtype=torch.float32),
                "actions": torch.tensor(item["actions"], dtype=torch.int64),
                "returns_to_go": torch.tensor(item["returns_to_go"], dtype=torch.float32),
                "timesteps": torch.tensor(item["timesteps"],_len - 1:
                # Ensure next step exists and has an action
                if t+1 < len(episode) and 'action' in episode[t+1]:
                    target_action = episode[t+1]['action']
                else:
                    logger.warning(f"Cannot find target action for step {t dtype=torch.int64),
                "attention_mask": torch.tensor(item["attention_mask"], dtype=torch.int64),
                "targets": torch.tensor(item["targets"], dtype=torch.int64)} # Targets used by Trainer implicitly if labels not removed

@dataclass
class DecisionTransformer} in episode {episode_idx}. Skipping.")
                    continue # Skip if target action is missing
            else:
                 continue

            padding_len = context_length - len(seq_states)
            try:
                padded_states = np.concatenate([np.zeros((padding_len, STATE_DIM), dtype=np.floatDataCollator:
    def __call__(self, features):
        batch = {}
        batch["states"] = torch.stack([f["states"] for f in features])
        batch["actions"] = torch.stack([f["actions"] for f in features])
        batch["returns_to_go"] = torch.stack([f["returns_to_go"] for f in features])
        batch["timesteps"] = torch.stack([f["timesteps"] for f in features])
        batch["attention_mask"] = torch.32), np.array(seq_states, dtype=np.float32)], axis=0)
                padded_actions = np.concatenate([np.full((padding_len, NUM_ACTION_DIMS), -100, dtype=np.int64), np.array(seq_actions, dtype=np.int64)], axis=0)
            except ValueError as ve:
                 logger.error(f"Error during padding/concatenation at ep {episode_idx}, t {t}: {ve}. Seq states shape: {np.array(seq_states).shape if seq_states else 'Empty'}, Seq actions shape: {np.array(stack([f["attention_mask"] for f in features])
        # The 'targets' key here is not directly used by the model's forward
        # but is used implicitly by the Trainer for labels if 'labels' isn't present
        # Let's keep it for now, prediction_step handles labels explicitly
        batch["targets"] = torch.stack([f["targets"] for f in features])
        return batch

# --- Custom Trainer with Metricsseq_actions).shape if seq_actions else 'Empty'}")
                 continue # Skip sequence if padding fails (likely due to inconsistent inner shapes)

            padded_rtgs = np.concatenate([np.zeros(padding_len, dtype=np.float32), seq_rtgs], axis=0)
            padded_timesteps = np.concatenate([np.zeros(padding_len, dtype=np.int64), seq_tim Computation ---
class MultiDiscreteDecisionTransformerTrainer(Trainer):
    def __init__(self, *, num_actions_per_dim=None, **kwargs):
        super().__init__(**kwargs)
        if num_actions_per_dim is None:
            raise ValueError("`num_actions_per_dim` must be provided")
        self.num_actions_per_dim = num_actions_per_dim
        self.num_action_dims = len(num_actions_per_dim)
        self.action_slice_esteps], axis=0)
            attention_mask = np.concatenate([np.zeros(padding_len, dtype=np.int64), np.ones(len(seq_states), dtype=np.int64)], axis=0)

            all_sequences.append({
                "states": padded_states, "actions": padded_actions,
                "returns_to_go": padded_rtgs.reshape(-1, 1),
                "timesteps": padded_timesteps.reshape(-1, 1),
                "attention_mask": attention_mask, "targets": target_action
            })

    logger.info(f"Created {len(all_sequences)} sequences.")
    logger.info(f"Maximum episode length found:starts = np.concatenate(([0], np.cumsum(self.num_actions_per_dim)[:-1]))
        self.concatenated_action_dim = sum(self.num_actions_per_dim)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # --- This method remains unchanged from the previous working version ---
        _ = kwargs
        original_actions_long = inputs["actions"]
        batch_size, seq_len, _ = original_actions_long.shape
        one_hot_actions_list = []
        for i in range(self.num_action_dims):
            action_dim_indices = original_actions_long[:, :, i].clone()
            padding_mask {max_ep_len}")

    # --- State Normalization Calculation ---
    if not all_states:
         # If all_states is empty, it means no valid observations were found.
         # Cannot calculate mean/std. Need to decide how to proceed.
         # Option 1: Raise error. Option 2: Return dummy stats (zeros).
         logger.error("No valid states collected. Cannot calculate normalization statistics.")
         raise ValueError("No valid states found in the provided episodes. Check STATE_DIM or data loading.")
         # Or: return all_sequences, max_dim = (action_dim_indices == -100)
            action_dim_indices[padding_mask_dim] = 0
            one_hot_dim = F.one_hot(action_dim_indices, num_classes=self.num_actions_per_dim[i])
            one_hot_dim[padding_mask_dim] = 0
            one_hot_actions_list.append(one_hot_dim)
        actions_for_model_input = torch.cat(one_hot_actions_list, dim=-1).float()
        timesteps_for_model_input = inputs["timesteps"].squeeze_ep_len, np.zeros(STATE_DIM), np.ones(STATE_DIM) # Dummy stats

    all_states_np = np.array(all_states)
    state_mean = np.mean(all_states_np, axis=0)
    state_std = np.std(all_states_np, axis=0) + 1e-6

    # --- Apply Normalization ---
    for seq in all_sequences:
        valid_state_mask = seq['attention_mask'] == 1
        # Apply normalization only where std is significantly > 0
        safe_std = np.where(state_std > 1e-6, state_std, 1.0) # Avoid division by zero
        normalized_states(-1)

        outputs = model(states=inputs["states"], actions=actions_for_model_input,
                        returns_to_go=inputs["returns_to_go"], timesteps=timesteps_for_model_input,
                        attention_mask=inputs["attention_mask"], return_dict=True)
        action_preds = outputs.action_preds
        act_dim = action_preds.shape[-1]
        all_logits = action_preds.view(-1, act_dim)
        target_actions_for_loss = original_actions_long.long()
        valid_action_mask = (target_actions_for_loss != -100).all(dim=-1)
        valid_action_mask_flat = valid_action_mask.view(-1)
        valid_logits = all_logits[valid_action_mask_flat]

        if valid_logits.shape[0] == 0:
             logger.warning("No valid actions in batch for loss calculation.")
             loss_val = torch.tensor(0.0, device=model. = (seq['states'][valid_state_mask] - state_mean) / safe_std
        seq['states'][valid_state_mask] = normalized_states
        seq['states'][~valid_state_mask] = 0.0 # Ensure padding is zero

    return all_sequences, max_ep_len, state_mean, state_std


# --- 2. Hugging Face Dataset and Collator ---
# (DecisionTransformerDataset and DecisionTransformerDataCollator remain the same)
class DecisionTransformerDataset(TorchDataset):
    def __init__(self, sequences):
        self.sequences = sequences

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        item = self.sequences[idx]
        return {
            "states": torch.tensor(item["states"], dtype=torch.float32),
            "actions": torch.tensor(item["actions"], dtype=torch.int64),
            "returnsdevice, requires_grad=True)
             return (loss_val, outputs) if return_outputs else loss_val

        valid_targets_indices = target_actions_for_loss[valid_action_mask]
        total_loss = 0
        criterion = nn.CrossEntropyLoss()
        for i in range(self.num_action_dims):
            start_idx = self.action_slice_starts[i]; end_idx = start_idx + self.num_actions_per_dim[i]
            dim_logits = valid_logits[:, start_idx:end_idx]
            dim_targets = valid_targets_indices[:, i].long()
            loss = criterion(dim_logits, dim_targets)
            total_loss += loss
        avg_loss = total_loss / self.num_action_dims
        return (avg_loss, outputs) if return_outputs else avg_loss

    def prediction_step( # --- This method remains unchanged ---
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None,
    ) -> Tuple[Optional[_to_go": torch.tensor(item["returns_to_go"], dtype=torch.float32),
            "timesteps": torch.tensor(item["timesteps"], dtype=torch.int64),
            "attention_mask": torch.tensor(item["attention_mask"], dtype=torch.int64),
            "targets": torch.tensor(item["targets"], dtype=torch.int64)
        }

@dataclass
class DecisionTransformerDataCollator:
    def __call__(self, features):
        batch = {}
        batch["states"] = torch.stack([f["states"] for f in features])
        batch["actions"] = torch.stack([f["actions"] for f in features])
        batch["returns_to_go"] = torch.stack([f["returns_to_go"] for f in features])
        batch["timesteps"] = torch.stack([f["timesteps"] for f in features])
        batch["attention_mask"] = torch.stack([f["attention_mask"] for f in features])
        batch["targets"] = torch.stack([f["targets"] for f in features])
        return batch

# --- 3. Compute Metrics Function ---

# Global variables needed by compute_metrics (can be refactored into Trainer state iftorch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        if ignore_keys is None: ignore_keys = []
        original_actions_long = inputs["actions"]
        batch_size, seq_len, _ = original_actions_long.shape
        one_hot_actions_list = []
        for i in range(self.num_action_dims):
            action_dim_indices = original_actions_long[:, :, i].clone()
            padding_mask_dim = (action_dim_indices == -100)
            action_dim_indices[padding_mask_dim] = 0
            one_hot_dim = F.one_hot(action_dim_indices, num_classes=self.num_actions_per_dim[i])
            one_hot_dim[padding_mask_dim] = 0
            one_hot_actions_list.append(one_hot_dim)
        actions_for_model_input = torch.cat(one_hot_actions_list, dim=-1).float()
        timesteps_for_model_input = inputs["timesteps"].squeeze(-1)
        model_inputs = {"states": inputs["states"], "actions": actions_for_model_input,
                        "returns_to_go": inputs["returns_to_go"], "timesteps": timesteps_for_model_input, needed)
# These are set within the Trainer subclass now for better encapsulation.

# --- 4. Custom Trainer ---
class MultiDiscreteDecisionTransformerTrainer(Trainer):
    def __init__(self, *, num_actions_per_dim=None, **kwargs):
        super().__init__(**kwargs)
        if num_actions_per_dim is None:
            raise ValueError("`num_actions_per_dim` must be provided")
        # Store these for compute_metrics and compute_loss
        self.num_actions_per_dim = num_actions_per_dim
        self.num_action_dims = len(num_actions_per_dim)
        self.action_slice_starts = np.concatenate(([0], np.cumsum(self.num_actions_per_dim)[:-1]))
        self.concatenated_action_dim = sum(self.num_actions_per_dim)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # (compute_loss method remains the same as the last working version)
        _ = kwargs # Avoid unused variable warning
        original_actions_long = inputs["actions"]

        batch_size, seq_len, _ = original_actions_long.shape
        one_hot_actions_list = []
        for i in range(self.num_action_dims):
            action_dim_indices = original_actions_long[:, :, i].clone()
            padding_mask_dim = (action_dim_
                        "attention_mask": inputs["attention_mask"], "return_dict": True,}
        loss = None; logits = None; labels = None
        with torch.no_grad():
            outputs = model(**model_inputs)
            if not prediction_loss_only:
                action_preds = outputs.action_preds; act_dim = action_preds.shape[-1]
                all_logits = action_preds.view(-1, act_dim)
                target_actions_for_loss = original_actions_long.long()
                valid_action_mask = (target_actions_for_loss != -100).all(dim=-1)
                valid_action_mask_flat = valid_action_mask.view(-1)
                valid_logits = all_logits[valid_action_mask_flat]
                if valid_logits.shape[0] > 0:
                    valid_targets_indices = target_actions_for_loss[valid_action_mask]
                    total_loss = 0; criterion = nn.CrossEntropyLoss()
                    for i in range(self.num_action_dims):
                        start_idx = self.action_slice_starts[i]; end_idx = start_idx + self.num_actions_per_dim[i]
                        dim_logits = valid_logits[:, start_idx:end_idx]
                        dim_targets = valid_targets_indices[:, i].long()
                        loss_dim = criterion(dim_logits, dim_targets); total_loss += loss_dim
                    loss = total_loss / self.num_action_dims
                else: loss = torch.tensor(0.0, device=model.device)
            logits = outputs.indices == -100)
            action_dim_indices[padding_mask_dim] = 0
            one_hot_dim = F.one_hot(action_dim_indices, num_classes=self.num_actions_per_dim[i])
            one_hot_dim[padding_mask_dim] = 0
            one_hot_actions_list.append(one_hot_dim)
        actions_for_model_input = torch.cat(one_hot_actions_list, dim=-1).float()

        timesteps_for_model_input = inputs["timesteps"].squeeze(-1)

        outputs = model(
            states=inputs["states"],
            actions=actions_for_model_input,
            returns_to_go=inputs["returns_to_go"],
            timesteps=timesteps_for_model_input,
            attention_mask=inputs["attention_mask"],
            return_dict=True,
        )

        action_preds = outputs.action_preds
        act_dim = action_preds.shape[-1]
        all_logits = action_preds.view(-1, act_dim)
        target_actions_for_loss = original_actions_long.long()
        valid_action_mask = (target_actions_for_loss != -100).all(dim=-1)
        valid_action_mask_flat = valid_action_mask.view(-1)
        valid_logits = all_logits[valid_action_mask_flat]

        if valid_logits.shape[0] == 0:
             logger.warning("No valid actions found in batch for loss calculation.")
             loss_val = torch.tensor(0.0, device=model.device, requires_grad=True)
             return (loss_val, outputs) if return_outputs else loss_val

        valid_targets_indices = target_actions_for_loss[valid_action_mask]
        total_loss = 0
        criterion = nn.CrossEntropyLoss()
        for i in range(self.num_action_dims):
            start_idx = self.action_slice_starts[i]
            end_idx = start_idx + self.num_actions_action_preds
            labels = original_actions_long # Use original actions as labels
        if loss is not None: loss = loss.detach()
        if logits is not None: logits = logits.detach()
        if labels is not None: labels = labels.detach()
        return (loss, logits, labels) # Ensure loss is returned!

    # --- ADD compute_metrics METHOD ---
    def compute_metrics(self, p: EvalPrediction):
        """Computes accuracy and F1 score for multi-discrete actions."""
        # p.predictions contains the logits from prediction_step
        # p.label_ids contains the labels from prediction_step (original_actions_long)
        logits = torch.from_numpy(p.predictions) # Shape: (batch, seq_len, concat_dim)
        labels = torch.from_numpy(p.label_ids)   # Shape: (batch, seq_len, num_dims)

        # Flatten and filter out padded steps (-100 labels)
        # IMPORTANT: Match the logic in compute_loss/prediction_step
        valid_action_mask = (labels != -100).all(dim=-1) # Shape: (batch, seq_len)
        valid_action_mask_flat = valid_action_mask.view(-1) # Shape: (batch * seq_len)

        flat_logits = logits.view(-1, self.concatenated_action_dim) # Shape: (batch * seq_len, concat_dim)
        flat_labels = labels.view(-1, self.num_action_dims)         # Shape: (batch * seq_len, num_dims)

        valid_logits = flat_logits[valid_action_mask_flat] # Shape: (num_valid, concat_dim)
        valid_labels = flat_labels[valid_action_mask_flat] # Shape: (num_valid, num_dims)

        if valid_per_dim[i]
            dim_logits = valid_logits[:, start_idx:end_idx]
            dim_targets = valid_targets_indices[:, i].long()
            loss = criterion(dim_logits, dim_targets)
            total_loss += loss
        avg_loss = total_loss / self.num_action_dims

        return (avg_loss, outputs) if return_outputs else avg_loss


    def prediction_step(
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None,
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        # (prediction_step method remains the same as the last working version)
        if ignore_keys is None: ignore_keys = []
        original_actions_long = inputs["actions"]

        batch_size, seq_len, _ = original_actions_long.shape
        one_hot_actions_list = []
        for i in range(self.num_action_dims):
            action_dim_indices = original_actions_long[:, :, i].clone()
            padding_mask_dim = (action_dim_indices == -100)
            action_dim_indices[padding_mask_dim] = 0
            one_hot_dim = F.one_hot(action_dim_indices, num_classes=self.num_actions_per_dim[i])
            one_hot_dim[padding_mask_dim] = 0
            one_hot_actions_list.append(one_hot_dim)
        actions_for_model_input = torch.cat(one_hot_actions_list, dim=-1).float()

        timesteps_for_model_input = inputs["timesteps"].squeeze(-1)

        model_inputs = {
            "states": inputs["states"], "actions": actions_for_model_input,
            "returns_to_go": inputs["returns_to_go"], "timesteps": timesteps_for_model_input,
            "attention_mask": inputs["attention_mask"], "return_dict": True,
        }

        loss = None; logits = None; labels = None
        labels.shape[0] == 0:
            logger.warning("No valid labels found in eval batch for metrics calculation.")
            # Return default values or empty dict
            return {"eval_accuracy": 0.0, "eval_f1_macro": 0.0}

        metrics = {}
        all_preds = []
        all_true = []
        total_correct_actions = 0
        num_valid_timesteps = valid_labels.shape[0]

        # Calculate predictions and metrics per dimension
        for i in range(self.num_action_dims):
            start_idx = self.action_slice_starts[i]
            end_idx = start_idx + self.num_actions_per_dim[i]

            # Get predictions for this dimension
            dim_logits = valid_logits[:, start_idx:end_idx]
            dim_preds = torch.argmax(dim_logits, dim=-1)

            # Get true labels for this dimension
            dim_true = valid_labels[:, i]

            # Store for overall calculation if needed
            all_preds.append(dim_preds.cpu().numpy())
            all_true.append(dim_true.cpu().numpy())

            # Calculate metrics for this dimension
            acc = accuracy_score(dim_true.cpu().numpy(), dim_preds.cpu().numpy())
            # Use macro F1 for unweighted average across classes within the dimension
            # Set zero_division=0 to handle cases where a class isn't predicted/present
            f1 = f1_score(dim_true.cpu().numpy(), dim_preds.cpu().numpy(), average='macro', zero_division=0)

            metrics[f"eval_accuracy_dim_{i}"] = acc
            metrics[f"eval_f1_macro_dim_{i}"] = f1

        # Calculate overall accuracy (all dimensions must match)
        allwith torch.no_grad():
            outputs = model(**model_inputs)
            if not prediction_loss_only:
                action_preds = outputs.action_preds
                act_dim = action_preds.shape[-1]
                all_logits = action_preds.view(-1, act_dim)
                target_actions_for_loss = original_actions_long.long()
                valid_action_mask = (target_actions_for_loss != -100).all(dim=-1)
                valid_action_mask_flat = valid_action_mask.view(-1)
                valid_logits = all_logits[valid_action_mask_flat]

                if valid_logits.shape[0] > 0:
                    valid_targets_indices = target_actions_for_loss[valid_action_mask]
                    total_loss = 0
                    criterion = nn.CrossEntropyLoss()
                    for i in range(self.num_action_dims):
                        start_idx = self.action_slice_starts[i]
                        end_idx = start_idx + self.num_actions_per_dim[i]
                        dim_logits = valid_logits[:, start_idx:end_idx]
                        dim_targets = valid_targets_indices[:, i].long()
                        loss_dim = criterion(dim_logits, dim_targets)
                        total_loss += loss_dim_preds_np = np.stack(all_preds, axis=1) # Shape: (num_valid, num_dims)
        all_true_np = np.stack(all_true, axis=1)   # Shape: (num_valid, num_dims)

        # Check equality row-wise (all dimensions must match for the timestep action to be correct)
        correct_actions_mask = np.all(all_preds_np == all_true_np, axis=1)
        overall_accuracy = np.sum(correct_actions_mask) / num_valid_timesteps if num_valid_timesteps > 0 else 0.0
        metrics["eval_accuracy"] = overall_accuracy # Overall accuracy

        # Calculate overall F1 is less straightforward for multi-discrete multi-label
        # We can average the per-dimension F1 scores
        avg_f1_macro = np.mean([metrics[f"eval_f1_macro_dim_{i}"] for i in range(self.num_action_dims)])
        metrics["eval_f1_macro"] = avg_f1_macro

        # IMPORTANT: Ensure 'eval_loss' is present if metric_for_best
                    loss = total_loss / self.num_action_dims
                else:
                    loss = torch.tensor(0.0, device=model.device)

            logits = outputs.action_preds # Raw logits from model (batch, seq, concat_dim)
            labels = original_actions_long # Original action indices (batch, seq, num_dims)

        if loss is not None: loss = loss.detach()
        if logits is not None: logits = logits.detach()
        if labels is not None: labels = labels.detach()
        # Important: Ensure loss is returned for 'eval_loss' to be automatically tracked
        return (loss, logits, labels)


# --- 5. Compute Metrics Function ---
def compute_metrics(eval_pred: EvalPrediction):
    """Computes accuracy and F1 score for multi-discrete actions."""
    # eval_pred.predictions == logits (batch, seq_len, concatenated_action_dim)
    # eval_pred.label_ids == labels == original_actions_long (batch, seq_len, num_action_dims)
    logits, labels = eval__model is 'eval_loss'
        # The Trainer *should* add this automatically if prediction_step returns loss.
        # If the KeyError persists, uncomment and adapt the following:
        # loss_val = self.prediction_step(self.model, inputs_for_eval, prediction_loss_only=False)[0] # Re-run to get loss? (inefficient)
        # if loss_val is not None:
        #     metrics["eval_loss"] = loss_val.item() # Or extract from Trainer state if possible

        return metricspred.predictions, eval_pred.label_ids

    if logits is None or labels is None:
        logger.warning("Logits or labels missing in EvalPrediction, cannot compute metrics.")
        return {}

    # Ensure labels are integers
    labels = labels.astype(np.int64)

    # Reshape and find valid (non-padded) steps
    # Logits shape: (batch*seq_len, concatenated_action_dim)
    # Labels shape: (batch*seq_len, num_action_dims)
    num_action_dims = labels.shape[-1] # Should be 3
    logits_flat = logits.reshape(-1, logits.shape[-1])
    labels_flat = labels.reshape(-1, num_action_dims)

    # Mask based on labels padding value (-100)
    valid_mask = (labels_flat != -100).all(axis=-1)
    valid_logits = logits_flat[valid_mask]
    valid_labels
    # --- END compute_metrics METHOD ---


# --- 4. Training Function ---

def train(pickle_path, model_name, cumulative_rewards_in_dataset=False):
    # ... (Load and Process Data - same as before) ...
    # 1. Load and Process Data
    raw_episodes = load_your_data(pickle_path, cumulative_rewards_in_dataset)
    if not raw_episodes: raise ValueError("Failed to load episodes.")
    split_idx = int(len(raw_episodes) * 0.9)
    train_episodes = raw_episodes[:split_idx]; eval_episodes = labels_flat[valid_mask]

    if valid_logits.shape[0] == 0:
        logger.warning("No valid (non-padded) samples found in evaluation batch for metrics.")
        return {} # = raw_episodes[split_idx:]
    logger.info(f"Split: {len(train_episodes)} train, {len(eval_episodes)} eval episodes.")
    if not train_episodes: raise Return empty if no valid samples

    metrics = {}
    action_slice_starts = np.concatenate(([0], np.cumsum(NUM_ACTIONS_PER_DIM)[:-1])) # Needs NUM_ACTIONS_PER_DIM

    # Calculate metrics per dimension
    all_preds_match = [] # For exact match accuracy
    for i in range ValueError("No training episodes after split.")

    # Process Training Data
    train_sequences, max_ep_len_train(num_action_dims):
        start_idx = action_slice_starts[i]
        end_idx, state_mean, state_std = process_dataset(train_episodes, CONTEXT_LENGTH)
    if not train_sequences: raise ValueError("Training data processing resulted in zero sequences.")
    logger.info(f" = start_idx + NUM_ACTIONS_PER_DIM[i]

        # Get predictions for this dimension
        dimUsing State Mean (first 5): {state_mean[:5]}")
    logger.info(f"Using State Std_logits = valid_logits[:, start_idx:end_idx]
        dim_preds = np.argmax(dim_ (first 5): {state_std[:5]}")

    # Process Evaluation Data
    if eval_logits, axis=-1)

        # Get true labels for this dimension
        dim_labels = valid_labels[:, i]episodes:
        eval_sequences, max_ep_len_eval, _, _ = process_dataset(eval_

        # Store if prediction matched for exact match calculation
        if i == 0:
            all_preds_match.episodes, CONTEXT_LENGTH)
        for seq in eval_sequences: # Apply training normalization
            validappend(dim_preds == dim_labels)
        else:
            # Element-wise AND with previous dimension = seq['attention_mask'] == 1
            norm_s = np.where(state_std > 1 matches
            all_preds_match[0] = np.logical_and(all_preds_match[0e-6, (seq['states'][valid] - state_mean) / state_std, 0.], dim_preds == dim_labels)

        # Calculate accuracy
        acc = accuracy_score(dim_labels, dim0)
            seq['states'][valid] = norm_s; seq['states'][~valid] = _preds)
        metrics[f"eval_acc_dim{i}"] = acc

        # Calculate F0.0
        logger.info(f"Processed {len(eval_sequences)} evaluation sequences.")
    1 score (macro avg handles imbalance between action choices)
        f1 = f1_score(dim_labels, dimelse: logger.warning("No evaluation episodes."); eval_sequences = []; max_ep_len_eval = 0

_preds, average='macro', zero_division=0)
        metrics[f"eval_f1_    max_ep_len = max(max_ep_len_train, max_ep_len_eval)
    if max_ep_len == 0: max_ep_len = 500;dim{i}"] = f1

    # Calculate exact match accuracy
    exact_match_accuracy = np.mean logger.warning(f"Max ep len 0, setting default: {max_ep_len}")

    train_(all_preds_match[0]) if all_preds_match else 0.0
    metrics["dataset = DecisionTransformerDataset(train_sequences)
    eval_dataset = DecisionTransformerDataset(eval_sequences) if evaleval_acc_exact_match"] = exact_match_accuracy

    # Note: 'eval_loss' is typically_sequences else None

    # 2. Configure Model
    concatenated_action_dim = sum(NUM added automatically by the Trainer
    # if prediction_step returns the loss as the first element.
    return metrics


# ---_ACTIONS_PER_DIM)
    config = DecisionTransformerConfig(state_dim=STATE_DIM, act 6. Plotting Function ---
def plot_metrics(log_history, save_dir, model_name):_dim=concatenated_action_dim, hidden_size=128,
                                       n_layer=
    """Plots training and evaluation metrics from Trainer log history."""
    logger.info(f"Plotting metrics and3, n_head=4, activation_function="relu", dropout=0.1,
                                       n_inner=None, max_length=CONTEXT_LENGTH, max_ep_len=max_ep_len saving to: {save_dir}")
    os.makedirs(save_dir, exist_ok=True) # Ensure,
                                       action_tanh=False)
    model = DecisionTransformerModel(config); model.to( directory exists

    train_logs = [log for log in log_history if 'loss' in log] # Training logsDEVICE)

    # Create output/log directories
    output_dir = os.path.join('examples', 'Agents', 'DT', 'output', f"{model_name}-output")
    logging_dir = os.path usually have 'loss' key
    eval_logs = [log for log in log_history if 'eval_loss'.join('examples', 'Agents', 'DT', 'logs', f"{model_name}-logs")
     in log] # Eval logs have 'eval_loss' key

    if not train_logs and not eval_logs:
results_dir = os.path.join('examples', 'Agents', 'DT', 'Results')
    final        logger.warning("No training or evaluation logs found in history. Cannot plot metrics.")
        return

    # Extract_model_path = os.path.join(results_dir, f"{model_name}-final")
 steps and metrics
    train_steps = [log['step'] for log in train_logs]
    train_    os.makedirs(output_dir, exist_ok=True); os.makedirs(logging_dir, exist_ok=True); os.makedirs(results_dir, exist_ok=True)

    # 3loss = [log['loss'] for log in train_logs]
    eval_steps = [log['step'] for log in eval_logs]
    eval_loss = [log['eval_loss'] for log in eval. Set up Trainer Arguments - RE-ENABLE load_best_model
    training_args = TrainingArguments(
        output__logs]

    # --- Plot Loss ---
    plt.figure(figsize=(10, 6))dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=
    if train_steps and train_loss:
        plt.plot(train_steps, train_loss, label='10,
        per_device_train_batch_size=32,
        per_device_eval_batchTraining Loss', marker='.')
    if eval_steps and eval_loss:
        plt.plot(eval_steps_size=32,
        learning_rate=1e-4,
        weight_decay=1, eval_loss, label='Validation Loss (BC Loss)', marker='o')
    plt.xlabel('Steps')
e-4,
        warmup_ratio=0.1,
        logging_dir=logging_dir    plt.ylabel('Loss')
    plt.title(f'{model_name} - Training and Validation Loss,      # <<< TensorBoard logs stored here
        logging_strategy="steps",     # Log metrics every logging_steps
        ')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.pathlogging_steps=100,
        save_strategy="steps",
        save_steps=500,
.join(save_dir, f"{model_name}_loss_plot.png"))
    plt.close()

            evaluation_strategy="steps" if eval_dataset else "no", # Evaluate every eval_steps
        eval_steps=500 if eval_dataset else None,
        load_best_model_at_end=True if# --- Plot Accuracy & F1 ---
    num_dims = NUM_ACTION_DIMS # Get number of dimensions eval_dataset else False, # <<< Re-enable
        metric_for_best_model="eval_loss
    fig, axes = plt.subplots(num_dims + 1, 1, figsize=(10, " if eval_dataset else None, # <<< Use eval_loss (should now be present)
        greater_is_better6 * (num_dims + 1)), sharex=True) # +1 for exact match
    if=False,      # Lower eval_loss is better
        remove_unused_columns=False,
        # num_dims == 0: # Handle case with only exact match plot if needed
         axes = [axes] # Make report_to="tensorboard", # Explicitly enable tensorboard reporting (optional, often default)
    )

    collator it iterable

    # Per-dimension plots
    for i in range(num_dims):
        eval_acc_key = DecisionTransformerDataCollator()
    trainer = MultiDiscreteDecisionTransformerTrainer(
        model=model,
 = f'eval_acc_dim{i}'
        eval_f1_key = f'eval_f1_dim{i}'
        if eval_acc_key in eval_logs[0] and eval_f1_key in        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval eval_logs[0]: # Check if keys exist
            acc_values = [log[eval_acc_key] for_dataset,
        data_collator=collator,
        num_actions_per_dim=NUM log in eval_logs]
            f1_values = [log[eval_f1_key] for_ACTIONS_PER_DIM,
        compute_metrics=lambda p: trainer.compute_metrics(p), # Pass the method itself
    )

    # 4. Train
    logger.info(f"Starting training for { log in eval_logs]
            ax = axes[i]
            ax.plot(eval_steps, accmodel_name}...")
    logger.info(f" TensorBoard logs will be saved in: {logging_dir}")
_values, label=f'Val Accuracy Dim {i}', marker='o', linestyle='-')
            ax    logger.info(" Run `tensorboard --logdir examples/Agents/DT/logs` to view logs.")
    trainer.plot(eval_steps, f1_values, label=f'Val F1-Score (Macro) Dim {i.train()

    # 5. Save final model and stats
    logger.info(f"Saving final model}', marker='x', linestyle='--')
            ax.set_ylabel('Score')
            ax.set_title(f'Dimension {i} Metrics')
            ax.legend()
            ax.grid(True)
 (best={training_args.load_best_model_at_end}) to: {final_model_        else:
             logger.warning(f"Metrics '{eval_acc_key}' or '{eval_fpath}")
    trainer.save_model(final_model_path)
    logger.info(f"Saving normalization stats1_key}' not found in eval logs.")
             axes[i].set_title(f'Dimension {i} to: {final_model_path}/normalization_stats.npz")
    np.savez(f Metrics (Not Found)')
             axes[i].grid(True)


    # Exact Match Accuracy Plot
    eval"{final_model_path}/normalization_stats.npz", mean=state_mean, std=state__exact_acc_key = 'eval_acc_exact_match'
    ax_exact = axes[std, max_ep_len=max_ep_len)

    logger.info(f"Training finishednum_dims] # Last subplot
    if eval_exact_acc_key in eval_logs[0]:
 for {model_name}.")
    return state_mean, state_std, max_ep_len


# --- Main        exact_acc_values = [log[eval_exact_acc_key] for log in eval_logs Execution Block ---
if __name__ == "__main__":
    # ... (Keep the main block same as previous]
        ax_exact.plot(eval_steps, exact_acc_values, label='Val Accuracy (Exact Match version to set parameters and call train) ...
    WEIGHT = 0.1
    REWARD_TYPE = 'ar)', marker='s', linestyle='-')
        ax_exact.set_ylabel('Accuracy')
        ax_exact.set_title('Exact Match Accuracy')
        ax_exact.legend()
        ax_exactousal'
    DATA_SOURCE = 'PPO'
    if WEIGHT == 0: LABEL = 'Optimize'
    .grid(True)
    else:
        logger.warning(f"Metric '{eval_exact_acc_key}'elif WEIGHT == 0.5: LABEL = 'Blended'
    else: LABEL = 'Arousal not found in eval logs.")
        ax_exact.set_title('Exact Match Accuracy (Not Found)')
'

    DATASET_PATH = os.path.join('examples', 'Agents', DATA_SOURCE, 'datasets',        ax_exact.grid(True)

    # Common X label
    axes[-1].set_xlabel('Steps') f'{DATA_SOURCE}_{LABEL}_{REWARD_TYPE}_SolidObs_dataset.pkl')
    MODEL_NAME = f"{DATA_SOURCE}_{LABEL}_{REWARD_TYPE}_SolidObs_DT"
    CUMULATIVE_ # Set x-label only on the bottom-most plot
    fig.suptitle(f'{model_name} - ValidationREWARDS = False # <<<--- Set True if needed

    logger.info(f"--- Training Config ---")
     Accuracy and F1 Scores', y=1.02) # Overall title
    plt.tight_layout(logger.info(f" Model Name: {MODEL_NAME}")
    logger.info(f" Dataset:rect=[0, 0, 1, 1]) # Adjust layout
    plt.savefig(os.path {DATASET_PATH}")
    logger.info(f" State Dim: {STATE_DIM}")
    .join(save_dir, f"{model_name}_accuracy_f1_plot.png"))
    logger.info(f" Cumulative Rewards: {CUMULATIVE_REWARDS}")
    logger.info(plt.close()

    logger.info("Finished plotting metrics.")


# --- 7. Training Function ---

f" Device: {DEVICE}")
    logger.info(f"----------------------")

    try:
        statedef train(pickle_path, model_name, cumulative_rewards_in_dataset=False):
    #_mean, state_std, max_ep_len = train(
            pickle_path=DATASET_ 1. Load and Process Data
    raw_episodes = load_your_data(pickle_path, cumulative_rewards_in_dataset)
    if not raw_episodes:
        raise ValueError("Failed toPATH, model_name=MODEL_NAME,
            cumulative_rewards_in_dataset=CUMULATIVE_REWARDS load any episodes.")

    split_idx = int(len(raw_episodes) * 0.9
        )
        logger.info(f"Training complete for {MODEL_NAME}.")
    except FileNotFoundError: logger)
    train_episodes = raw_episodes[:split_idx]
    eval_episodes =.error(f"Dataset not found: {DATASET_PATH}")
    except ValueError as ve: logger.error raw_episodes[split_idx:]
    logger.info(f"Split: {len(train_episodes(f"ValueError during setup: {ve}")
    except Exception as e: logger.error(f"Unexpected)} train, {len(eval_episodes)} eval episodes.")

    if not train_episodes:
 training error: {e}", exc_info=True)


# --- END OF FILE trainObsDT.py ---
         raise ValueError("No training episodes after split.")

    # Process Training Data (get normalization stats)
    train_```

**Modifications for `evaluateObsDT.py`**

We will add optional plotting using Matplotlib aftersequences, max_ep_len_train, state_mean, state_std = process_dataset(
        train_episodes, CONTEXT_LENGTH
    )
    if not train_sequences:
         raise ValueError the evaluation loop finishes.

```python
# --- START OF FILE evaluateObsDT.py ---

import torch
("Training data processing resulted in zero sequences.")
    logger.info(f"Using State Mean: {state_mean[:5from transformers import (
    DecisionTransformerConfig,
    DecisionTransformerModel,
)
import numpy as np
]}...")
    logger.info(f"Using State Std: {state_std[:5]}...")

    # Process Evaluationimport random
import logging
import os
import argparse
# --- Add Matplotlib import ---
import matplotlib.pyplot as plt Data
    if eval_episodes:
        eval_sequences, max_ep_len_eval, _,
# --- End import ---

try:
    from affectively_environments.envs.solid_game_obs import _ = process_dataset(
            eval_episodes, CONTEXT_LENGTH
        )
        for seq in eval_ SolidEnvironmentGameObs
    ENV_AVAILABLE = True
except ImportError:
    logger.warning("SolidEnvironmentGameObs notsequences: # Apply train stats to eval data
            valid_state_mask = seq['attention_mask'] ==  found.")
    ENV_AVAILABLE = False

# --- Configuration ---
logging.basicConfig(level=logging.INFO)1
            safe_std = np.where(state_std > 1e-6, state_std, 
logger = logging.getLogger(__name__)

NUM_ACTIONS_PER_DIM = [3, 3, 2]
NUM_ACTION_DIMS = len(NUM_ACTIONS_PER_DIM)
STATE_DIM = 1.0)
            normalized_states = (seq['states'][valid_state_mask] - state_54 # <<<--- Ensure this matches the trained model's state_dim
CONTEXT_LENGTH = 20
DEVICEmean) / safe_std
            seq['states'][valid_state_mask] = normalized_states
             = "cuda" if torch.cuda.is_available() else "cpu"

# --- Evaluation Function ---
seq['states'][~valid_state_mask] = 0.0
        logger.info(f"def evaluate_online(env_creator, model_path, target_rtg, num_episodes=1Processed {len(eval_sequences)} evaluation sequences.")
    else:
        logger.warning("No evaluation episodes after split.")
        eval_sequences = []
        max_ep_len_eval = 0

    max_0, plot=False): # Add plot flag
    """Evaluates the trained model online in the environment with optionalep_len = max(max_ep_len_train, max_ep_len_eval)
     plotting."""

    logger.info(f"Starting online evaluation: Target RTG={target_rtg}, Episodesif max_ep_len == 0: max_ep_len = 500

    train_dataset = Decision={num_episodes}")
    logger.info(f"Loading model from: {model_path}")
TransformerDataset(train_sequences)
    eval_dataset = DecisionTransformerDataset(eval_sequences) if eval_    if not model_path or not os.path.exists(model_path):
         logger.error(f"Modelsequences else None

    # 2. Configure Model
    concatenated_action_dim = sum(NUM_ path not found: {model_path}"); return None, None

    # Load model and stats
    try:ACTIONS_PER_DIM)
    config = DecisionTransformerConfig(
        state_dim=STATE_DIM,
        model_config = DecisionTransformerConfig.from_pretrained(model_path)
        if model_config.state act_dim=concatenated_action_dim,
        hidden_size=128, n_layer=3, n_head=4, activation_function="relu",
        dropout=0.1, n_dim != STATE_DIM:
            logger.error(f"STATE_DIM mismatch! Model={model_config.state_inner=None, max_length=CONTEXT_LENGTH,
        max_ep_len=max_ep_dim}, Script={STATE_DIM}.")
            return None, None
        model = DecisionTransformerModel.from_len, action_tanh=False,
    )
    model = DecisionTransformerModel(config)
    _pretrained(model_path); model.to(DEVICE).eval()
        stats_path = os.pathmodel.to(DEVICE)

    # Create output/log directories
    output_dir = os.path.join('.join(model_path, "normalization_stats.npz")
        if not os.path.exists(stats_examples', 'Agents', 'DT', 'output', f"{model_name}-output")
    logging_dirpath):
             logger.error(f"Stats file not found: {stats_path}"); return None, None
        stats = os.path.join('examples', 'Agents', 'DT', 'logs', f"{model_name}- = np.load(stats_path)
        state_mean = stats['mean']; state_std = statslogs")
    results_dir = os.path.join('examples', 'Agents', 'DT', 'Results['std']; max_ep_len = int(stats['max_ep_len'])
        if state_')
    final_model_path = os.path.join(results_dir, f"{model_name}-mean.shape[0] != STATE_DIM or state_std.shape[0] != STATE_DIM:final")
    plot_save_dir = os.path.join(results_dir, f"{model_name}-final
             logger.error(f"Stats dimension mismatch! Expected {STATE_DIM}, got mean={state_mean", "plots") # Subdir for plots

    os.makedirs(output_dir, exist_ok=True)
    .shape}, std={state_std.shape}.")
             return None, None
    except Exception as e:os.makedirs(logging_dir, exist_ok=True)
    os.makedirs(results_dir,
         logger.error(f"Error loading model/stats: {e}", exc_info=True); return None, exist_ok=True)
    # Plot dir created by plot_metrics

    # 3. Set up Trainer None

    _num_actions_per_dim = NUM_ACTIONS_PER_DIM; _num_action
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dims = len(_num_actions_per_dim)
    _action_slice_starts = np._dir=True,
        num_train_epochs=10, # Adjust as needed
        per_device_concatenate(([0], np.cumsum(_num_actions_per_dim)[:-1]))
    _concatenated_actiontrain_batch_size=32,
        per_device_eval_batch_size=32,
        _dim = sum(_num_actions_per_dim)

    # Create environment
    try: env = env_creatorlearning_rate=1e-4,
        weight_decay=1e-4,
        warmup(); logger.info(f"Created environment: {type(env)}")
    except Exception as e: logger.error(_ratio=0.1,
        logging_dir=logging_dir, # Crucial for TensorBoard
        logging_f"Failed to create env: {e}", exc_info=True); return None, None

    episode_returns = []steps=100,
        save_strategy="steps",
        save_steps=500,
        evaluation_strategy="steps" if eval_dataset else "no",
        eval_steps=50
    episode_lengths = []
    # --- Store data for plotting ---
    all_ep_target_rtgs =0 if eval_dataset else None,
        load_best_model_at_end=False, # Keep []
    all_ep_rewards = []
    # --- End plot data storage ---

    for ep in range( disabled unless eval_loss issue resolved
        metric_for_best_model=None,
        greater_is_betternum_episodes):
        logger.info(f"--- Starting Eval Episode {ep+1}/{num_ep=False,
        remove_unused_columns=False,
        report_to=["tensorboard"], # Explicitisodes} ---")
        try:
            reset_output = env.reset()
            state = reset_outputly report to TensorBoard
    )

    collator = DecisionTransformerDataCollator()
    trainer = Multi[0] if isinstance(reset_output, tuple) else reset_output
            state = np.array(DiscreteDecisionTransformerTrainer(
        model=model, args=training_args, train_dataset=train_dataset,
state, dtype=np.float32)
            if state.shape != (STATE_DIM,):
        eval_dataset=eval_dataset, data_collator=collator,
        compute_metrics=compute                logger.error(f"Initial state shape mismatch! Got {state.shape}"); continue
        except Exception as e: logger_metrics, # Pass the metrics function
        num_actions_per_dim=NUM_ACTIONS_PER_DIM, #.error(f"Env reset failed ep {ep+1}: {e}", exc_info=True); continue

 Pass needed info
    )

    # 4. Train
    logger.info(f"Starting training for        done = False; ep_return = 0.0; ep_len = 0
        current_ model: {model_name}")
    trainer.train()

    # 5. Save final model and statstarget_rtg = float(target_rtg)
        # Store per-episode plot data
        ep
    logger.info(f"Saving final model to: {final_model_path}")
    trainer._target_rtgs = [current_target_rtg]
        ep_rewards = []

        try:save_model(final_model_path)
    logger.info(f"Saving normalization stats to: {final # Initialize context buffers
            norm_state = (state - state_mean) / state_std
            context_states_model_path}/normalization_stats.npz")
    np.savez(f"{final_model = [np.zeros_like(state, dtype=np.float32)] * (CONTEXT_LENGTH -_path}/normalization_stats.npz", mean=state_mean, std=state_std, max_ 1) + [norm_state]
            context_actions_one_hot = [np.zeros(_ep_len=max_ep_len)

    # --- 6. Plot Metrics AFTER Training ---
    if trainer.state.log_history:
        plot_metrics(trainer.state.log_history, plot_saveconcatenated_action_dim, dtype=np.float32)] * CONTEXT_LENGTH
            context__dir, model_name)
    else:
        logger.warning("Trainer log history is empty, skippingrtgs = [np.array([current_target_rtg], dtype=np.float32)] * CONTEXT_LENGTH
            context_timesteps = [np.array([0], dtype=np.int6 plotting.")

    logger.info(f"Training finished for {model_name}.")
    # Return stats needed4)] * CONTEXT_LENGTH
        except Exception as e: logger.error(f"Context buffer init error by evaluation script if called programmatically (optional)
    # return state_mean, state_std, max_ep ep {ep+1}: {e}", exc_info=True); continue

        while not done:
            #_len


# --- Main Execution Block ---
if __name__ == "__main__":

    # --- Configuration for Prepare tensors
            try:
                states_tensor = torch.tensor(np.array(context_states)[-CONTEXT this Training Run ---
    WEIGHT = 0.1 # Example weight
    REWARD_TYPE = 'arousal'_LENGTH:], dtype=torch.float32).unsqueeze(0).to(DEVICE)
                actions_tensor = torch # Example reward type: 'arousal', 'score', or 'blended'
    DATA_SOURCE = '.tensor(np.array(context_actions_one_hot)[-CONTEXT_LENGTH:], dtype=torch.float32PPO' # 'PPO' or 'Explore'

    # Determine label based on weight
    if WEIGHT).unsqueeze(0).to(DEVICE)
                rtgs_tensor = torch.tensor(np.array( == 0: LABEL = 'Optimize'
    elif WEIGHT == 0.5: LABEL = 'Blendedcontext_rtgs)[-CONTEXT_LENGTH:], dtype=torch.float32).unsqueeze(0).to('
    else: LABEL = 'Arousal'

    # Construct paths and model name
    DATASET_PATH =DEVICE)
                current_context_timesteps_list = [ts.item() for ts in context_tim os.path.join('examples', 'Agents', DATA_SOURCE, 'datasets', f'{DATA_SOURCE}_{esteps[-CONTEXT_LENGTH:]]
                timesteps_np = np.array(current_context_timesteps_list, dtype=np.int64)
                timesteps_tensor = torch.tensor(timestepsLABEL}_{REWARD_TYPE}_SolidObs_dataset.pkl')
    MODEL_NAME = f"{DATA_SOURCE}_{LABEL_np, dtype=torch.long).unsqueeze(0).to(DEVICE)
                attn_mask_tensor = torch}_{REWARD_TYPE}_SolidObs_DT" # Consistent model name

    CUMULATIVE_REWARDS = False.ones_like(timesteps_tensor).to(DEVICE)
            except Exception as e: logger.error # <<<--- Set True if your dataset stores cumulative rewards per step

    logger.info(f"--- Training Configuration(f"Tensor creation error step {ep_len}: {e}", exc_info=True); break

            # Get action ---")
    logger.info(f"Model Name: {MODEL_NAME}")
    logger.info(
            try:
                with torch.no_grad():
                    outputs = model(states=states_tensor, actionsf"Dataset Path: {DATASET_PATH}")
    logger.info(f"State Dimension: {STATE=actions_tensor, returns_to_go=rtgs_tensor,
                                    timesteps=timesteps_tensor_DIM}") # Log the state dim being used
    logger.info(f"Cumulative Rewards in Dataset: {CUMULATIVE_REWARDS}")
    logger.info(f"Device: {DEVICE}")
    logger, attention_mask=attn_mask_tensor, return_dict=True)
                    logits = outputs.action_.info(f"-----------------------------")

    # Start Training
    try:
        train( # Removed assignmentpreds[0, -1]
            except Exception as e: logger.error(f"Model forward pass error step {ep as train doesn't explicitly return stats anymore
            pickle_path=DATASET_PATH,
            model_name=MODEL_len}: {e}", exc_info=True); break

            # Decode action
            predicted_action_indices = np.zeros(_num_action_dims, dtype=np.int64)
            predicted__NAME,
            cumulative_rewards_in_dataset=CUMULATIVE_REWARDS
        )
action_one_hot = np.zeros(_concatenated_action_dim, dtype=np.float32        logger.info(f"Training and plotting complete for {MODEL_NAME}.")
    except FileNotFoundError:
        )
            for i in range(_num_action_dims):
                start_idx = _action_slicelogger.error(f"Dataset not found at {DATASET_PATH}. Please ensure the path is correct and the file exists.")
    except ValueError as ve:
        logger.error(f"ValueError during training setup: {ve_starts[i]; end_idx = start_idx + _num_actions_per_dim[i]}", exc_info=True) # Show traceback for ValueErrors
    except Exception as e:
        logger.
                dim_logits = logits[start_idx:end_idx]; action_idx = torch.argmax(error(f"An unexpected error occurred during training: {e}", exc_info=True)


# --- END OFdim_logits).item()
                predicted_action_indices[i] = action_idx
                predicted_action_one_hot[start_idx + action_idx] = 1.0